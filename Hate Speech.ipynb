{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class                                              tweet\n",
      "0      2  !!! RT @mayasolovely: As a woman you shouldn't...\n",
      "1      1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
      "2      1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
      "3      1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
      "4      1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('labeled_data.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#display the first few rows of tweet row\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove text inside square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    # Remove newline characters\n",
    "    text = re.sub('\\n', '', text)\n",
    "\n",
    "    # Remove words containing digits\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'tweet' column\n",
    "data['clean_text'] = data['tweet'].apply(clean_text)\n",
    "\n",
    "#display the first few rows of tweet row\n",
    "print(data[['clean_text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download NLTK data files (necessary for first-time use)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to tokenize text into n-grams of given size\n",
    "def tokenize_text_ngrams(text, n=1): #change n to desired grams\n",
    "    tokens = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "# Apply tokenization to the 'clean_text' column\n",
    "data['tokens'] = data['clean_text'].apply(tokenize_text_ngrams)\n",
    "\n",
    "# Display the first few rows with tokenized n-grams\n",
    "print(data[['tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Add 'RT' to list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('rt')\n",
    "stop_words.add('im')\n",
    "stop_words.add('like')\n",
    "stop_words.add('dont')\n",
    "stop_words.add('got')\n",
    "stop_words.add('get')\n",
    "stop_words.add('u')\n",
    "stop_words.add('aint')\n",
    "\n",
    "# Function to filter stop words from tokens\n",
    "def filter_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply stop word filtering to the 'tokens' column\n",
    "data['filtered_tokens'] = data['tokens'].apply(filter_stopwords)\n",
    "\n",
    "# Display the first few rows with filtered tokens\n",
    "data[['filtered_tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Flatten the list of all words to get word frequencies\n",
    "all_words = [word for tokens in data['filtered_tokens'] for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Get the top 10 most common words\n",
    "most_common_words = word_freq.most_common(10)\n",
    "\n",
    "# Create bar plot for the top 10 words\n",
    "words, counts = zip(*most_common_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts, color='blue')\n",
    "plt.title('Top 10 Most Common Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Most Common Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply the lemmatization function to the tokenized tweets\n",
    "data['text'] = data['filtered_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "#rename columns\n",
    "data.rename(columns={'class': 'label'}, inplace=True)\n",
    "\n",
    "# Select only the 'class' and 'lemmatized_tokens' columns\n",
    "df = data[['label', 'text']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('df.csv', index=False)\n",
    "\n",
    "# Display sorted DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIWC Incel Violent Extremism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        IVED\n",
      "0       0.00\n",
      "1      11.11\n",
      "2      11.11\n",
      "3      25.00\n",
      "4      10.00\n",
      "...      ...\n",
      "24769   0.00\n",
      "24770   0.00\n",
      "24771   0.00\n",
      "24772  20.00\n",
      "24773   0.00\n",
      "\n",
      "[24774 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "inputFileCSV = r\"C:\\Users\\Gigabyte\\OneDrive\\Documents\\Jobs\\NTU\\DETER\\Models\\Hate Speech\\df.csv\"\n",
    "outputLocation = r\"C:\\Users\\Gigabyte\\OneDrive\\Documents\\Jobs\\NTU\\DETER\\Models\\Hate Speech\\LIWC\\Output\\Incel_Violent_Extremism_Output.csv\"\n",
    "\n",
    "liwcDict = r\"C:\\Users\\Gigabyte\\OneDrive\\Documents\\Jobs\\NTU\\DETER\\Models\\Hate Speech\\LIWC\\Dictionary\\incel-violent-extremism-dictionary.dicx\"\n",
    "\n",
    "cmd_to_execute = [\"LIWC-22-cli\",\n",
    "                  \"--mode\", \"wc\",\n",
    "                  \"--input\", inputFileCSV,\n",
    "                  \"--dictionary\", liwcDict,\n",
    "                  \"--row-id-indices\", \"1\",\n",
    "                  \"--column-indices\", \"2\",\n",
    "                  \"--output\", outputLocation]\n",
    "\n",
    "# Let's go ahead and run this analysis:\n",
    "subprocess.call(cmd_to_execute)\n",
    "\n",
    "# Load your dataset\n",
    "Dict1 = pd.read_csv(r\"C:\\Users\\Gigabyte\\OneDrive\\Documents\\Jobs\\NTU\\DETER\\Models\\Hate Speech\\LIWC\\Output\\Incel_Violent_Extremism_Output.csv\")\n",
    "\n",
    "# Select only the 'IVED' column\n",
    "print(Dict1[['IVED']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to undo n-grams\n",
    "def undo_ngrams(ngrams_list):\n",
    "    return ' '.join([str(gram) for gram in ngrams_list])\n",
    "\n",
    "# Apply undo_ngrams function using .loc to avoid SettingWithCopyWarning\n",
    "df.loc[:, 'text'] = df['text'].apply(undo_ngrams)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "final_df = pd.concat([df[['text']], Dict1[['IVED']], df[['label']]], axis=1)\n",
    "\n",
    "# Replace NaN values\n",
    "final_df.fillna({'text': '', 'IVED': 0}, inplace=True)  # Replace NaN in 'text' with '' and in 'IVED' with 0\n",
    "\n",
    "#if we're using TPOT\n",
    "#df[['text']]\n",
    "\n",
    "# Take a sample of 10% of the data\n",
    "#final_df.sample(frac=0.1)\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "#print(final_df_sample)\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(final_df)\n",
    "\n",
    "# Class count\n",
    "count_class_0, count_class_1, count_class_2 = final_df['label'].value_counts()\n",
    "\n",
    "# display label count\n",
    "count_class_0, count_class_1, count_class_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification with TF-IDF & \n",
    "Ensembling: Random Forest, Gradient boosting machines, Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "\n",
    "# Ensure text data is in DataFrame format\n",
    "X = final_df[['text', 'IVED']]  # Convert to DataFrame to ensure 2D structure\n",
    "y = final_df['label']\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "# Splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Define TF-IDF vectorizer with unigrams only\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Column transformer to apply TF-IDF only to the 'text' column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', tfidf_vectorizer, 'text'),\n",
    "        ('other', 'passthrough', ['IVED'])  # Keep other columns as they are\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define base models for ensemble\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "\n",
    "# Define Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('gb', gb_model),\n",
    "    ('mlp', mlp_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Define the pipeline with preprocessor and Voting Classifier\n",
    "pipeline_voting = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('voting', voting_classifier)\n",
    "])\n",
    "\n",
    "# Evaluate the model using RepeatedStratifiedKFold\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "#cross_val_scores = cross_val_score(pipeline_voting, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "#print(f\"Cross-validated accuracy: {cross_val_scores.mean():.4f} (+/- {cross_val_scores.std():.4f})\")\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline_voting.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(pipeline_voting, filename)\n",
    "\n",
    "# Predictions using Voting Classifier\n",
    "y_pred_voting = pipeline_voting.predict(X_test)\n",
    "\n",
    "# Evaluate Voting Classifier\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f\"Voting Model Accuracy: {accuracy_voting:.4f}\")\n",
    "\n",
    "print(\"\\nVoting Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_voting, zero_division=0))\n",
    "\n",
    "print(\"\\nVoting Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Load the saved model\n",
    "filename = 'finalized_model.sav'\n",
    "pipeline_voting = joblib.load(filename)\n",
    "\n",
    "# Function to clean and preprocess new text data\n",
    "def preprocess_new_data(new_texts, liwc_output):\n",
    "    # Cleaning function\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        return text\n",
    "\n",
    "    # Apply cleaning\n",
    "    cleaned_texts = [clean_text(text) for text in new_texts]\n",
    "\n",
    "    # Tokenize text\n",
    "    nltk.download('punkt')\n",
    "    tokenized_texts = [word_tokenize(text) for text in cleaned_texts]\n",
    "\n",
    "    # Filter stopwords\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['rt', 'im', 'like', 'dont', 'got', 'get', 'u', 'aint'])\n",
    "    filtered_texts = [[word for word in tokens if word not in stop_words] for tokens in tokenized_texts]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_texts = [[lemmatizer.lemmatize(token) for token in tokens] for tokens in filtered_texts]\n",
    "\n",
    "    # Undo n-grams\n",
    "    def undo_ngrams(ngrams_list):\n",
    "        return ' '.join([str(gram) for gram in ngrams_list])\n",
    "\n",
    "    preprocessed_texts = [undo_ngrams(tokens) for tokens in lemmatized_texts]\n",
    "\n",
    "    # Create DataFrame with 'text' and 'IVED' columns\n",
    "    new_data = pd.DataFrame({'text': preprocessed_texts, 'IVED': liwc_output})\n",
    "\n",
    "    return new_data\n",
    "\n",
    "# Interactive prompt for input text\n",
    "input_text = 'how are you?'\n",
    "\n",
    "# Example new data\n",
    "new_texts = [input_text]\n",
    "\n",
    "# LIWC dictionary location\n",
    "liwc_dict = r\"C:\\Users\\Gigabyte\\OneDrive\\Documents\\Jobs\\NTU\\DETER\\Models\\Hate Speech\\LIWC\\Dictionary\\incel-violent-extremism-dictionary.dicx\"\n",
    "\n",
    "# Command to execute LIWC analysis\n",
    "cmd_to_execute = [\"LIWC-22-cli\",\n",
    "                  \"--mode\", \"wc\",\n",
    "                  \"--input\", \"console\",\n",
    "                  \"--dictionary\", liwc_dict,\n",
    "                  \"--console-text\", input_text,\n",
    "                  \"--output\", \"console\"]\n",
    "\n",
    "# Run the LIWC analysis and parse the result\n",
    "results = subprocess.check_output(cmd_to_execute, shell=True).strip().splitlines()\n",
    "results_json = json.loads(results[1])\n",
    "liwc_output = [results_json['IVED']]  # Replace with actual IVED value\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data = preprocess_new_data(new_texts, liwc_output)\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = pipeline_voting.predict(new_data)\n",
    "\n",
    "# Display the predictions\n",
    "labels = {0: \"Hate Speech\", 1: \"Offensive Language\", 2: \"Neither\"}\n",
    "predicted_label = labels[predictions[0]]\n",
    "\n",
    "print(\"Predictions:\", predicted_label)\n",
    "print(liwc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split\n",
    "\n",
    "# Example DataFrame (use your actual final_df)\n",
    "# final_df = pd.read_csv('your_data.csv')  # Assuming final_df is read from a CSV or created previously\n",
    "\n",
    "# summarize the dataset\n",
    "# split into input and output elements\n",
    "data = final_df.values\n",
    "X, y = data[:, 0], data[:, -1]  # Assuming the text is in the first column and labels are in the last column\n",
    "\n",
    "# Reshape X to make it 2D (assuming X is text data and y is labels)\n",
    "X = X.reshape(-1, 1)  # This assumes each element in X is a single text string; adjust accordingly if needed\n",
    "\n",
    "# Define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Define TPOT sparse classifier\n",
    "model = TPOTClassifier(generations=5, population_size=50, cv=cv, scoring='accuracy', verbosity=2, random_state=1, n_jobs=-1)\n",
    "\n",
    "# perform the search\n",
    "model.fit(X, y)\n",
    "\n",
    "# export the best model\n",
    "model.export('tpot_best_model.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
